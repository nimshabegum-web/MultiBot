# Chatbot Capstone - Full-Stack AI Chatbot

A production-ready, full-stack chatbot with **SSE streaming**, **session persistence**, and **multi-provider model switching**.

## ğŸš€ Features

- **Real-time streaming** responses with token-by-token display
- **Session management** with persistent chat history
- **Multi-provider support**: OpenAI, Anthropic, Gemini, Ollama (BONUS)
- **Modern React UI** with ChatGPT-like interface
- **FastAPI backend** with async streaming endpoints
- **Postgres database** for data persistence
- **Rate limiting** and token caps for cost control

## ğŸ—ï¸ Architecture

```
Frontend (React + Vite) â†â†’ Backend (FastAPI) â†â†’ AI Providers
                              â†“
                        Postgres Database
```

## ğŸ“¦ Quick Start

### 1. Clone & Setup
```bash
git clone <your-repo>
cd chatbot-capstone
```

### 2. Backend Setup
```bash
cd backend
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Database Setup
```bash
# Start Postgres (from project root)
docker compose up -d

# Or use your own Postgres instance
# Update DATABASE_URL in .env
```

### 4. Environment Configuration
```bash
# Copy and fill your API keys
cp env.example .env
# Edit .env with your actual API keys
```

### 5. Start Backend
```bash
cd backend
export PYTHONPATH=.
uvicorn app:app --host 0.0.0.0 --port 8000 --reload
```

### 6. Start Frontend
```bash
cd frontend
npm install
npm run dev
```

Visit: http://localhost:5173

## ğŸ”§ Configuration

### Environment Variables
- `OPENAI_API_KEY`: Your OpenAI API key
- `ANTHROPIC_API_KEY`: Your Anthropic API key  
- `GEMINI_API_KEY`: Your Google Gemini API key
- `OLLAMA_BASE_URL`: Local Ollama instance (optional)
- `DATABASE_URL`: Postgres connection string
- `MAX_OUTPUT_TOKENS`: Token limit (default: 1024)

### AI Providers
- **OpenAI**: GPT-4o-mini, GPT-4o-mini-translate
- **Anthropic**: Claude 3.5 Haiku
- **Gemini**: Gemini 1.5 Flash
- **Ollama**: Local models (llama3, phi3, qwen2)

## ğŸ§ª Testing

### Test SSE Streaming
```bash
curl -N -H "Accept: text/event-stream" \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Hello!"}],"provider":"openai","model":"gpt-4o-mini"}' \
  http://localhost:8000/chat/<session_id>/stream
```

### API Endpoints
- `GET /models` - Available providers and models
- `POST /sessions` - Create new chat session
- `GET /sessions` - List user sessions
- `GET /sessions/{id}` - Get session messages
- `POST /chat/{id}/stream` - Stream chat responses (SSE)

## ğŸ¯ Assignment Requirements

This boilerplate covers all **mandatory requirements**:

âœ… **SSE Streaming** - Real-time token-by-token responses  
âœ… **Session Management** - Users, sessions, message persistence  
âœ… **Multi-Provider Switching** - OpenAI + Anthropic/Gemini  
âœ… **Modern UI/UX** - ChatGPT-like interface  
âœ… **Database Persistence** - Postgres with SQLAlchemy  
âœ… **Rate Limiting** - Configurable token caps  
âœ… **Environment Security** - .env for API keys  

## ğŸš€ Deployment

### Local Development
- Backend: `uvicorn app:app --reload`
- Frontend: `npm run dev`
- Database: `docker compose up -d`

### Production Build
```bash
# Frontend
cd frontend
npm run build

# Backend
cd backend
uvicorn app:app --host 0.0.0.0 --port 8000
```

## ğŸ” Project Structure

```
chatbot-capstone/
â”œâ”€ backend/                 # FastAPI application
â”‚  â”œâ”€ app.py              # Main API endpoints
â”‚  â”œâ”€ providers/          # AI provider implementations
â”‚  â”œâ”€ models.py           # Database models
â”‚  â”œâ”€ schemas.py          # Pydantic schemas
â”‚  â””â”€ requirements.txt    # Python dependencies
â”œâ”€ frontend/              # React application
â”‚  â”œâ”€ src/
â”‚  â”‚  â”œâ”€ components/      # React components
â”‚  â”‚  â”œâ”€ hooks/          # Custom hooks (useSSE)
â”‚  â”‚  â””â”€ api/            # API client
â”‚  â””â”€ package.json       # Node dependencies
â”œâ”€ docker-compose.yml     # Postgres setup
â””â”€ env.example           # Environment template
```

## ğŸ‰ Bonus Features

- **Ollama Integration**: Local OSS models
- **Markdown Rendering**: Rich message display
- **Copy to Clipboard**: Easy message copying
- **Responsive Design**: Mobile-friendly interface

## ğŸ“š Next Steps

1. **Customize UI**: Modify colors, fonts, layout
2. **Add Features**: File uploads, code highlighting
3. **Enhance Security**: User authentication, rate limiting
4. **Deploy**: AWS, Vercel, or your preferred platform

---

**Happy Coding! ğŸš€**

This boilerplate gives you a solid foundation to build upon. All the complex parts (SSE, providers, database) are already implemented - focus on your unique features and polish!
